train <- nba_cla %>% filter(sample == TRUE)
test <- nba_cla %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
modelsvm = svm(Win ~., train, cost=0.005, kernel = "linear")
predYsvm = predict(modelsvm, test)
misClassError(test$Win, predYsvm)
## no correlation - ts
nba_cla <- nba %>%
select(8,
14,15,16,18,19,21,24,30,34,35,37,40,43,
49,50,51,53,54,56,59,65,69,70,72,75,78)
set.seed(214)
sample <- sample.split(nba_cla$Win, SplitRatio = .70)
train <- nba_cla %>% filter(sample == TRUE)
test <- nba_cla %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
modelsvm = svm(Win ~., train, cost=0.005, kernel = "linear")
predYsvm = predict(modelsvm, test)
misClassError(test$Win, predYsvm)
#### 2022 Model Training ####
library(tidyverse)
library(caTools)
library(readxl)
library(tidymodels)
library(broom)
library(ggfortify)
library(glmnet)
library(caret)
library(InformationValue)
library(class)
library(e1071)
rm(list=ls())
# nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_noadj.xlsx")
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj.xlsx")
View(nba)
## no correlation - eFG
nba_reg <- nba %>%
select(8,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
## no correlation - ts
nba_reg <- nba %>%
select(8,
14,15,16,18,19,21,24,30,34,35,37,40,43,
49,50,51,53,54,56,59,65,69,70,72,75,78)
set.seed(214)
sample <- sample.split(nba_reg$Win, SplitRatio = .70)
train <- nba_reg %>% filter(sample == TRUE)
test <- nba_reg %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
summary(train)
library(rms)
install.packages("rms")
library(rms)
## no correlation - eFG
nba_cla <- nba %>%
select(8,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
set.seed(214)
sample <- sample.split(nba_cla$Win, SplitRatio = .70)
train <- nba_cla %>% filter(sample == TRUE)
test <- nba_cla %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
log_mod <- glm(Win ~., data = train, family = "binomial")
cal <- rms::calibrate(log_mod)
log_mod <- lrm(Win ~., data = train, family = "binomial")
log_mod <- lrm(Win ~., data = train)
cal <- rms::calibrate(log_mod)
log_mod <- lrm(Win ~., data = train, x=T, y=T)
cal <- rms::calibrate(log_mod)
plot(cal)
plot(cal, legend = T)
log_mod <- lrm(Win ~., data = train, x=T, y=T, legend = TRUE)
log_mod <- lrm(Win ~., data = train, x=T, y=T, legend = TRUE)
cal <- rms::calibrate(log_mod, legend = TRUE)
plot(cal)
cal <- rms::calibrate(log_mod, subtitles = TRUE)
plot(cal)
cal <- rms::calibrate(log_mod, legend = TRUE,
subtitles = T)
plot(cal)
cal <- rms::calibrate(log_mod, legend=T)
plot(cal)
cal <- rms::calibrate(log_mod, legend=T)
plot(cal)
plot(cal, legend=T)
cal <- rms::calibrate(log_mod, LEGEND=T)
plot(cal, legend=T)
plot(cal)
## no correlation - eFG
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
## no correlation - ts
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,24,30,34,35,37,40,43,
49,50,51,53,54,56,59,65,69,70,72,75,78)
set.seed(214)
sample <- sample.split(nba_reg$Margin, SplitRatio = .70)
train <- nba_reg %>% filter(sample == TRUE)
test <- nba_reg %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
lin_mod <- ols(Margin ~., data = train, x=T, y=T)
cal <- rms::calibrate(lin_mod)
plot(cal)
cal <- rms::calibrate(lin_mod, legend=TRUE)
plot(cal)
cal <- rms::calibrate(lin_mod, method='boot')
plot(cal)
cal <- rms::calibrate(lin_mod)
plot(cal)
legend(x=.6, y=.4, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
plot(cal, legend=FALSE)
legend(x=.6, y=.4, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=.6, y=.4, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=10, y=-15, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=0, y=-5, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
plot(cal, legend=FALSE)
legend(x=0, y=-5, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
plot(cal, legend=T)
plot(cal, legend=T)
legend(x=0, y=-5, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=0, y=-5, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
plot(cal, legend=T)
log_mod <- lrm(Win ~., data = train, x=T, y=T)
cal <- rms::calibrate(log_mod)
log_mod <- lrm(Win ~., data = train, x=T, y=T)
## no correlation - eFG
nba_cla <- nba %>%
select(8,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
set.seed(214)
sample <- sample.split(nba_cla$Win, SplitRatio = .70)
train <- nba_cla %>% filter(sample == TRUE)
test <- nba_cla %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
log_mod <- lrm(Win ~., data = train, x=T, y=T)
cal <- rms::calibrate(log_mod)
plot(cal, legend=T)
legend(x=0, y=0, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
plot(cal, legend=T)
plot(cal, legend=F)
legend(x=0, y=0, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=.6, y=.4, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=.6, y=0, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
plot(cal, legend=F)
legend(x=.6, y=0, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
legend(x=.6, y=.6, legend=c("Apparent", "Bias-corrected", "Ideal"),
lty=c(3, 1, 2), bty="n")
#### 2022 Model Training ####
library(tidyverse)
library(caTools)
library(readxl)
library(tidymodels)
library(broom)
library(ggfortify)
library(glmnet)
library(caret)
library(InformationValue)
library(class)
library(e1071)
rm(list=ls())
# nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_noadj.xlsx")
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj.xlsx")
## no correlation - eFG
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
View(nba_reg)
set.seed(214)
sample <- sample.split(nba_reg$Margin, SplitRatio = .70)
train <- nba_reg %>% filter(sample == TRUE)
test <- nba_reg %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
lin_mod <- lrm(Margin ~., data = train, x=T, y=T)
log_mod <- rms::lrm(Win ~., data = train, x=T, y=T)
lin_mod <- rms::lrm(Margin ~., data = train, x=T, y=T)
cal <- rms::calibrate(lin_mod)
plot(cal, legend=T)
lin_mod <- rms::ols(Margin ~., data = train, x=T, y=T)
cal <- rms::calibrate(lin_mod)
plot(cal, legend=T)
lin_mod <- rms::orm(Margin ~., data = train, x=T, y=T)
cal <- rms::calibrate(lin_mod)
View(lin_mod)
lin_mod <- rms::ols(Margin ~., data = train, x=T, y=T)
cal <- rms::calibrate(lin_mod)
View(lin_mod)
val <- rms::validate(lin_mod)
plot(val)
lin_mod <- rms::ols(Margin ~., data = train, x=T, y=T)
cal <- rms::calibrate(lin_mod)
plot(cal, legend=T)
source("~/.active-rstudio-document", echo=TRUE)
#### 2022 Model Training ####
library(tidyverse)
library(caTools)
library(readxl)
library(tidymodels)
library(broom)
library(ggfortify)
library(glmnet)
library(caret)
library(InformationValue)
library(class)
library(e1071)
rm(list=ls())
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj_pos_fta.xlsx")
## no correlation - eFG
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
set.seed(214)
sample <- sample.split(nba_reg$Margin, SplitRatio = .70)
train <- nba_reg %>% filter(sample == TRUE)
test <- nba_reg %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
lin_mod <- lm(Margin ~., data = train)
summary(lin_mod)
#Step 1 - create the evaluation metrics function
eval_metrics <- function(model, df, predictions, target) {
resids = df[,target] - predictions
resids2 = resids**2
N = length(predictions)
r2 = as.character(round(summary(model)$r.squared, 4))
adj_r2 = as.character(round(summary(model)$adj.r.squared, 4))
print(adj_r2) #Adjusted R-squared
print(as.character(round(sqrt(sum(resids2)/N), 4))) #RMSE
}
# Step 2 - predicting and evaluating the model on train data
predictions = predict(lin_mod, newdata = train)
eval_metrics(lin_mod, train, predictions, target = 'Margin')
# Step 3 - predicting and evaluating the model on test data
predictions = predict(lin_mod, newdata = test)
eval_metrics(lin_mod, test, predictions, target = 'Margin')
### Ridge & Lasso models
dummies <- dummyVars(Margin ~ ., data = nba_reg)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)
print(dim(train_dummies)); print(dim(test_dummies))
x = as.matrix(train_dummies)
y_train = train$Margin
x_test = as.matrix(test_dummies)
y_test = test$Margin
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
Rsquare = R_square,
RMSE = RMSE
)
}
### Ridge model
lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
par(mfrow = c(1, 2))
plot(ridge_reg)
plot(ridge_reg, xvar = "lambda", label = TRUE)
summary(ridge_reg)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
plot(cv_ridge)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
eval_results(y_train, predictions_train, train)
# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y_test, predictions_test, test)
# Lasso model
lambdas <- 10^seq(2, -3, by = -.1)
# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5,
family='gaussian', type.measure='mse')
plot(lasso_reg)
plot(lasso_reg$glmnet.fit, xvar="lambda", label=TRUE)
cat('Min Lambda: ', lasso_reg$lambda.min, '\n 1Sd Lambda: ', lasso_reg$lambda.1se)
df_coef <- round(as.matrix(coef(lasso_reg, s=lasso_reg$lambda.min)), 2)
# See all contributing variables
df_coef[df_coef[, 1] != 0, ]
# Best
lambda_best <- lasso_reg$lambda.min
lambda_best
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE, family='gaussian')
predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, train)
predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, test)
# Elastic Net
# Set training control
train_cont <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
search = "random",
verboseIter = TRUE)
# Train the model
elastic_reg <- train(Margin ~ .,
data = train,
method = "glmnet",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = train_cont)
# Best tuning parameter
elastic_reg$bestTune
# Make predictions on training set
predictions_train <- predict(elastic_reg, x)
eval_results(y_train, predictions_train, train)
# Make predictions on test set
predictions_test <- predict(elastic_reg, x_test)
eval_results(y_test, predictions_test, test)
#### 2022 Model Training ####
library(tidyverse)
library(caTools)
library(readxl)
library(tidymodels)
library(broom)
library(ggfortify)
library(glmnet)
library(caret)
library(InformationValue)
library(class)
library(e1071)
rm(list=ls())
# nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_noadj.xlsx")
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj.xlsx")
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj_pos_fta.xlsx")
## no correlation - eFG
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
## no correlation - ts
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,24,30,34,35,37,40,43,
49,50,51,53,54,56,59,65,69,70,72,75,78)
set.seed(214)
sample <- sample.split(nba_reg$Margin, SplitRatio = .70)
train <- nba_reg %>% filter(sample == TRUE)
test <- nba_reg %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
summary(train)
lin_mod <- lm(Margin ~., data = train)
summary(lin_mod)
#Step 1 - create the evaluation metrics function
eval_metrics <- function(model, df, predictions, target) {
resids = df[,target] - predictions
resids2 = resids**2
N = length(predictions)
r2 = as.character(round(summary(model)$r.squared, 4))
adj_r2 = as.character(round(summary(model)$adj.r.squared, 4))
print(adj_r2) #Adjusted R-squared
print(as.character(round(sqrt(sum(resids2)/N), 4))) #RMSE
}
# Step 2 - predicting and evaluating the model on train data
predictions = predict(lin_mod, newdata = train)
eval_metrics(lin_mod, train, predictions, target = 'Margin')
# Step 3 - predicting and evaluating the model on test data
predictions = predict(lin_mod, newdata = test)
eval_metrics(lin_mod, test, predictions, target = 'Margin')
### Ridge & Lasso models
dummies <- dummyVars(Margin ~ ., data = nba_reg)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)
print(dim(train_dummies)); print(dim(test_dummies))
x = as.matrix(train_dummies)
y_train = train$Margin
x_test = as.matrix(test_dummies)
y_test = test$Margin
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
Rsquare = R_square,
RMSE = RMSE
)
}
### Ridge model
lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
par(mfrow = c(1, 2))
plot(ridge_reg)
plot(ridge_reg, xvar = "lambda", label = TRUE)
summary(ridge_reg)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
plot(cv_ridge)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
eval_results(y_train, predictions_train, train)
# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y_test, predictions_test, test)
# Lasso model
lambdas <- 10^seq(2, -3, by = -.1)
# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5,
family='gaussian', type.measure='mse')
plot(lasso_reg)
plot(lasso_reg$glmnet.fit, xvar="lambda", label=TRUE)
cat('Min Lambda: ', lasso_reg$lambda.min, '\n 1Sd Lambda: ', lasso_reg$lambda.1se)
df_coef <- round(as.matrix(coef(lasso_reg, s=lasso_reg$lambda.min)), 2)
# See all contributing variables
df_coef[df_coef[, 1] != 0, ]
# Best
lambda_best <- lasso_reg$lambda.min
lambda_best
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE, family='gaussian')
predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, train)
predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, test)
# Elastic Net
# Set training control
train_cont <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
search = "random",
verboseIter = TRUE)
#### 2022 Model Training ####
library(tidyverse)
library(caTools)
library(readxl)
library(tidymodels)
library(broom)
library(ggfortify)
library(glmnet)
library(caret)
library(InformationValue)
library(class)
library(e1071)
rm(list=ls())
# nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_noadj.xlsx")
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj.xlsx")
nba <- read_xlsx("/Users/Jesse/Documents/MyStuff/NBA Betting/NBAdb/NBAdb1722_oneadj_pos_fta.xlsx")
## no correlation - eFG
nba_reg <- nba %>%
select(7,
14,15,16,18,19,21,23,30,34,35,37,39,43,
49,50,51,53,54,56,58,65,69,70,72,74,78)
set.seed(214)
sample <- sample.split(nba_reg$Margin, SplitRatio = .70)
train <- nba_reg %>% filter(sample == TRUE)
test <- nba_reg %>% filter(sample == FALSE)
pre_proc_val <- preProcess(train[,-1], method = c("center", "scale"))
train[,-1] = predict(pre_proc_val, train[,-1])
test[,-1] = predict(pre_proc_val, test[,-1])
summary(train)
lin_mod <- lm(Margin ~., data = train)
summary(lin_mod)
#Step 1 - create the evaluation metrics function
eval_metrics <- function(model, df, predictions, target) {
resids = df[,target] - predictions
resids2 = resids**2
N = length(predictions)
r2 = as.character(round(summary(model)$r.squared, 4))
adj_r2 = as.character(round(summary(model)$adj.r.squared, 4))
print(adj_r2) #Adjusted R-squared
print(as.character(round(sqrt(sum(resids2)/N), 4))) #RMSE
}
# Step 2 - predicting and evaluating the model on train data
predictions = predict(lin_mod, newdata = train)
eval_metrics(lin_mod, train, predictions, target = 'Margin')
# Step 3 - predicting and evaluating the model on test data
predictions = predict(lin_mod, newdata = test)
eval_metrics(lin_mod, test, predictions, target = 'Margin')
